{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47aeec4c",
   "metadata": {},
   "source": [
    "# Generación de secuencias utilizando FaceMesh y CV2\n",
    "Carga coordenadas de mapeo desde archivo SVG para generar textura compatible con OpenSim / Second Life a partir de fotos de rostros obtenidos desde el Portapapeles del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importaciones requeridas\n",
    "\n",
    "import pyperclip\n",
    "from PIL import ImageGrab,Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, floor, sin, pi\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga archivo de coordenadas de referencias usadas para el mapeo desde archivo SVG\n",
    "\n",
    "indice_puntos_mapeo = []\n",
    "puntos_mapeo1=[]\n",
    "archivo = open(\"mapeo_sl_facemesh.svg\", \"rt\")\n",
    "sopa = BeautifulSoup( archivo, features = 'xml')\n",
    "archivo.close()\n",
    "# se determina factores de escalamiento\n",
    "final_width = int(sopa.svg['width'])\n",
    "final_height = int(sopa.svg['height'])\n",
    "final_viewBox = [float(x) for x in sopa.svg['viewBox'].split()]\n",
    "escala_x = final_width / ( final_viewBox[2] - final_viewBox[0] )\n",
    "escala_y = final_height / ( final_viewBox[3] - final_viewBox[1] )\n",
    "for punto in sopa.svg.find('g', id= 'layer3').find_all('rect'):\n",
    "    idx = punto['id'].split('_')\n",
    "    if len(idx) > 1:\n",
    "        indice_puntos_mapeo.append( int(idx[1]) )\n",
    "        puntos_mapeo1.append({ 'x':float(punto['x']) * escala_x , 'y':float(punto['y']) * escala_y })\n",
    "puntos_mapeo_df = pd.DataFrame(puntos_mapeo1, index = indice_puntos_mapeo)\n",
    "puntos_mapeo_df = puntos_mapeo_df.sort_index()\n",
    "puntos_mapeo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de Quads con areas de interes para el mapeo facial\n",
    "\n",
    "areas = [\n",
    "    #sobre las cejas\n",
    "    #[109, 107, 9, 10], [10, 9, 336, 338], [67, 66, 107, 109], [338, 336, 296,297], [103, 105, 66, 67],\n",
    "    #[297, 296, 334, 332], [54, 63, 105, 103], [332, 334, 293, 284], [162, 127, 143, 156], [162, 156, 63, 54],\n",
    "    #[284, 293, 383, 389], [383, 372, 356, 389],\n",
    "    # Orbitas\n",
    "    [156, 143, 130, 247], [63, 156, 247, 29], [105, 63, 29, 28], [105, 28, 190, 66], [66, 190, 122, 107],\n",
    "    [107, 122, 6, 9], [336, 9, 6, 351], [296, 336, 351, 414], [334, 296, 414, 258], [334, 258, 259, 293],\n",
    "    [293, 259, 467, 383], [383, 467, 359, 372], [372, 359, 255, 340], [255, 254, 347, 340], [252, 349, 347, 254],\n",
    "    [464, 465, 349, 252], [414, 351, 465, 464], [190, 244, 245, 122], [244, 22, 120, 245], [24, 118, 120, 22],\n",
    "    [25, 111, 118, 24], [143, 111, 25, 130],\n",
    "    #parpado izquierdo\n",
    "    [190, 173, 133, 244], [28, 158, 173, 190], [28, 29, 160, 158], [29, 247, 246, 160], [247, 130, 33, 246],\n",
    "    [130, 25, 7, 33], [7, 25, 24, 144], [153, 144, 24, 22], [133, 153, 22, 244],\n",
    "    #parpado derecho\n",
    "    [414, 464, 362, 398], [362, 464, 252, 380], [380, 252, 254, 373], [373, 254, 255, 249], [359, 263, 249, 255],\n",
    "    [467, 466, 263, 359], [259, 387, 466, 467], [258, 385, 387, 259], [258, 414, 398, 385],\n",
    "    # NAriz\n",
    "    [122, 245, 131, 51], [6, 122, 51, 5], [6, 5, 281, 351], [351, 281, 360, 465], [131, 129, 237, 51], [5, 51, 237, 1],\n",
    "    [5, 1, 457, 281], [360, 281, 457, 358], [237, 129, 98, 20], [237, 20, 94, 1], [457, 1, 94, 250], [457, 250, 327, 358],\n",
    "    [20, 98, 97, 94], [250, 94, 326, 327], [20, 97, 2, 94], [250, 94, 2, 326],\n",
    "    # alrededor boca\n",
    "    [129, 205, 206, 98], [358, 327, 426, 425], [205, 212, 57, 206], [425, 426, 287, 432], [98, 206, 57, 39],\n",
    "    [98, 39, 37, 97], [2, 97, 37, 0], [2, 0, 267, 326], [327, 326, 267, 269], [327, 269, 287, 426], [212, 204, 91, 57],\n",
    "    [91, 204, 194, 181], [181, 194, 201, 84], [84, 201, 200, 17], [314, 17, 200, 421], [405, 314, 421, 418],\n",
    "    [321, 405, 418, 424], [287, 321, 424, 432],\n",
    "    # Labios\n",
    "    [185, 57, 91, 78], [39, 185, 78, 81], [37, 39, 81, 82], [37, 82, 13, 0], [267, 0, 13, 312], [267, 312, 311, 269],\n",
    "    [269, 311, 308, 409], [409, 308, 321, 287], [308, 402, 405, 321], [317, 314, 405, 402], [317, 14, 17, 314],\n",
    "    [87, 84, 17, 14], [87, 178, 181, 84], [78, 91, 181, 178],\n",
    "    # Cachete izquierdo\n",
    "    #[127, 234, 111, 143], [111, 234, 132, 147], [132, 136, 212, 147], [111, 147, 205, 118], [147, 136, 212, 205],\n",
    "    [118, 205, 129, 120], [245, 120, 129, 131],\n",
    "    # Cachete derecho\n",
    "    #[356, 372, 340, 454], [340, 376, 361, 454], [361, 376, 432, 365], [340, 347, 425, 376], [425, 432, 365, 376],\n",
    "    [347, 349, 358, 425], [465, 360, 358, 349],         \n",
    "    # Menton\n",
    "    #[136, 149, 204, 212],[365, 432, 424, 378], [204, 149, 176, 194], [424, 418, 400, 378],\n",
    "    [194, 176, 148, 201], [201, 148, 152, 200], [200, 152, 377, 421], [418, 421, 377, 400],\n",
    "]\n",
    "\n",
    "# Mapeo de linea de difuminación\n",
    "\n",
    "lineas_alpha =[\n",
    "    [156, 143],  [143, 111], [111, 118], [118, 205], [205, 212], [212, 204], [204, 194], [194, 176], [176, 148],\n",
    "    [148, 152], [152, 377], [377, 400], [400, 418], [418, 424], [424, 432], [432, 425], [425, 347], [347, 340],\n",
    "    [340, 372], [372, 383],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo valida que las areas referencien todos los puntos recolectados\n",
    "\n",
    "def validar_puntos(etiqueta, lineas):\n",
    "    \"Valida que los indices de los puntos de un arreglo estén recolectados en el mapeo\"\n",
    "    \n",
    "    todos_puntos = list(set([ a for b in lineas for a in b]))\n",
    "    puntos_faltantes = [a for a in todos_puntos if a not in puntos_mapeo_df.index]\n",
    "    if len(puntos_faltantes) == 0:\n",
    "        print('{0}: Diagramación es consistente'.format(etiqueta))\n",
    "    else:\n",
    "        print('{1}: validar los siguientes puntos: {0}'.format(str(puntos_faltantes), etiqueta))\n",
    "        areas_problema = [ area for area in areas for punto in puntos_faltantes if punto in area ]\n",
    "        print(\"\\n\".join([str(area) for area in areas_problema] ))\n",
    "\n",
    "validar_puntos('Areas',areas)\n",
    "validar_puntos('Lineas Alfa',lineas_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcec79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para manipulacion de imágenes\n",
    "\n",
    "def imagen_portapapeles():\n",
    "    \"Devuelve la imagen contenida en el portapeles. Si no hay imagen, devuelve None\"\n",
    "    \n",
    "    try:\n",
    "        # Intentar abrir el contenido como una imagen\n",
    "        image = ImageGrab.grabclipboard()\n",
    "        \n",
    "        # Convertir la imagen a formato OpenCV (cv2)\n",
    "        image_cv2 = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        return image_cv2\n",
    "    \n",
    "    except :\n",
    "        # El contenido no es una imagen\n",
    "        return None\n",
    "    \n",
    "def mostrar_imagen(imagen):\n",
    "    \"Visualiza imagen BGR utilizando Matplotlib\"\n",
    "    \n",
    "    # Convertir la imagen de BGR a RGB\n",
    "    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "    # Mostrar la imagen utilizando matplotlib\n",
    "    plt.imshow(imagen_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def mostrar_imagen_gray(imagen):\n",
    "    \"Visualiza imagen sin realizar conversion utilizando Matplotlib\"\n",
    "    \n",
    "    # Mostrar la imagen utilizando matplotlib\n",
    "    plt.imshow(imagen)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ffb08",
   "metadata": {},
   "source": [
    "# Aquí comienza la acción\n",
    "Capturar con el portapapeles una imagen con un rostro antes de ejecutar el siguiente paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar teniendo imagen en el portapapeles\n",
    "\n",
    "imagen = imagen_portapapeles()\n",
    "if imagen is None:\n",
    "    print('No hay imagen en el portapapeles')\n",
    "else:\n",
    "    mostrar_imagen(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae las marcas en un arreglo\n",
    "\n",
    "results = None\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "    results = face_mesh.process(cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica marcas sobre copia \n",
    "\n",
    "annotated_image = np.copy(imagen)\n",
    "face_landmarks_list = results.multi_face_landmarks\n",
    "for idx in range(len(face_landmarks_list)):\n",
    "    print('Marcando marcas #{0}'.format(idx))\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    \n",
    "mostrar_imagen(annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma las landmarks en un Dataframe\n",
    "\n",
    "idx_marcas = 0\n",
    "face_landmarks = face_landmarks_list[idx_marcas]\n",
    "ancho = imagen.shape[1]\n",
    "alto = imagen.shape[0]\n",
    "puntos_landmarks = [ (idx,x.x,x.y) for idx,x in enumerate(face_landmarks.landmark) ]\n",
    "landmarks_df = pd.DataFrame( [{'x':q[1] * ancho, 'y':q[2] * alto } for q in puntos_landmarks] , index = [q[0] for q in puntos_landmarks] )\n",
    "landmarks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac06481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza cruce de dataframes para tener mapeo de coordenadas\n",
    "\n",
    "cruce_df = puntos_mapeo_df.join(landmarks_df, how='left', lsuffix='_destino', rsuffix='_origen')\n",
    "cruce_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza el remapeo desde la imagen origen a las posiciones de textura para OpenSimulator\n",
    "\n",
    "malla_inicial = np.zeros( imagen.shape, dtype=np.uint8 )\n",
    "malla_final = np.zeros( (final_height, final_width, 4), dtype=np.uint8 )\n",
    "imagen_alpha = cv2.cvtColor(imagen, cv2.COLOR_BGR2BGRA)\n",
    "for indice in range(0, len(areas)):\n",
    "    area = areas[indice]\n",
    "    \n",
    "    imagen_mascara = np.zeros( imagen_alpha.shape, dtype=np.uint8 )\n",
    "    mascara_recorte = np.zeros( (final_height, final_width) , dtype=np.uint8 )\n",
    "    \n",
    "    pol1 = [ (cruce_df.loc[ p ,'x_origen'], cruce_df.loc[ p ,'y_origen'])\n",
    "            for p in area\n",
    "    ]\n",
    "    pol2 = [ (cruce_df.loc[ p ,'x_destino'], cruce_df.loc[ p ,'y_destino'])\n",
    "            for p in area\n",
    "    ]\n",
    "        \n",
    "    pol1_int = np.array(pol1, dtype=np.int32)\n",
    "    pol2_int = np.array(pol2, dtype=np.int32)\n",
    "    pol1_float = np.array(pol1, dtype=np.float32)\n",
    "    pol2_float = np.array(pol2, dtype=np.float32)\n",
    "    \n",
    "    cv2.fillPoly( imagen_mascara, pts=[pol1_int], color=(255,255,255,255) )\n",
    "    cv2.polylines( imagen_mascara, [pol1_int], True, (255,255,255,255), 3 )\n",
    "    cv2.fillPoly( mascara_recorte, pts=[pol2_int], color=(255,) )\n",
    "    cv2.polylines( malla_inicial, [pol1_int], True, (255,255,255,255), 1 )\n",
    "\n",
    "    mascara_recorte_inv = cv2.bitwise_not(mascara_recorte)\n",
    "    \n",
    "    area_enmascarada = cv2.bitwise_and(imagen_mascara, imagen_alpha)\n",
    "    #if indice == 0:\n",
    "    #    mostrar_imagen(mascara_recorte)\n",
    "    \n",
    "    if len(area) < 4:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        matriz_transformacion = cv2.getPerspectiveTransform(pol1_float, pol2_float)\n",
    "        recorte = cv2.warpPerspective(area_enmascarada, matriz_transformacion, (final_width , final_height) )\n",
    "        #malla_final = cv2.bitwise_or(malla_final, recorte)\n",
    "        malla_final = cv2.add( cv2.bitwise_and(malla_final,malla_final,mask=mascara_recorte_inv) , \n",
    "                              cv2.bitwise_and(recorte,recorte,mask=mascara_recorte) )\n",
    "    except Exception as ex:\n",
    "        print(\"Error \\n{1}\\n{2}\\n{0}\\n\".format(str(ex), recorte.dtype, mascara_recorte.dtype ))\n",
    "        #print(\"Error \\n{1}\\n{2}\\n{0}\\n\".format(str(ex), str(pol1), str(pol2) ))\n",
    "\n",
    "# mostrar_imagen(malla_inicial)\n",
    "mostrar_imagen(malla_final)\n",
    "\n",
    "malla_2 = np.copy(malla_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traza lineas alpha en zonas de difuminado\n",
    "\n",
    "numero_lineas_alfa = 10\n",
    "malla_b, malla_g, malla_r, malla_a = cv2.split(malla_final)\n",
    "malla_a1 = np.copy(malla_a)\n",
    "for ancho in range(numero_lineas_alfa -1, -1,-1):\n",
    "    color = int((ancho + 1) * 256 / (numero_lineas_alfa + 1))\n",
    "    for linea in lineas_alpha:\n",
    "        cv2.line(malla_a1, ( int(cruce_df.loc[ linea[0], 'x_destino' ]), int(cruce_df.loc[ linea[0], 'y_destino' ] ) ), ( int(cruce_df.loc[ linea[1], 'x_destino' ]) , int(cruce_df.loc[ linea[1], 'y_destino' ]) ) , (color,) , (ancho + 1) * 5 )\n",
    "\n",
    "malla_a1 = cv2.bitwise_and(malla_a, malla_a1, mask = malla_a)\n",
    "\n",
    "malla_final = cv2.merge([malla_b, malla_g, malla_r, malla_a1])\n",
    "\n",
    "malla_2 = np.copy(malla_final)\n",
    "\n",
    "mostrar_imagen(malla_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifica la capa alfa en funcion de las tonalidades\n",
    "\n",
    "malla_b, malla_g, malla_r, malla_a = cv2.split(malla_final)\n",
    "malla_gray = cv2.cvtColor(malla_final, cv2.COLOR_BGRA2GRAY)\n",
    "tonos = np.array( list(set([ x for x in np.reshape(malla_gray, (final_width * final_height,) ) if x != 0 ])), dtype = np.uint8)\n",
    "#mediana = np.median(tonos)\n",
    "mediana = malla_gray[260, 543]\n",
    "#mediana = np.average(tonos)\n",
    "maximo = np.max(tonos)\n",
    "minimo = np.min(tonos)\n",
    "factor = 256 / (maximo - minimo)\n",
    "print('{0} {1} {2}'.format(minimo, mediana, maximo))\n",
    "malla_a1 = np.copy(malla_a)\n",
    "\n",
    "for fila in range(0,malla_a1.shape[0]):\n",
    "    for columna in range(0,malla_a1.shape[1]):\n",
    "        if malla_a[fila,columna] == 0:\n",
    "            continue\n",
    "        elif malla_gray[fila, columna] >= mediana:\n",
    "            malla_a1[fila,columna] = 255 * sin( (malla_gray[fila, columna] - mediana ) * pi / (maximo - mediana) )\n",
    "        else:\n",
    "            malla_a1[fila,columna] = 255 * sin( (mediana - malla_gray[fila, columna]) * pi / (mediana - minimo) )\n",
    "            \n",
    "malla_a1 = cv2.bitwise_and(malla_a, malla_a1)\n",
    "\n",
    "malla_2 = cv2.merge([malla_b, malla_g, malla_r, malla_a1])\n",
    "        \n",
    "mostrar_imagen(malla_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b61b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graba la imagen obtenida\n",
    "\n",
    "nombre_archivo = input(\"Nombre de archivo .png: \")\n",
    "archivo_nombre = \"{0}_face.png\".format(nombre_archivo)\n",
    "cv2.imwrite(archivo_nombre, malla_2)\n",
    "print(\"{0} ha sido creado\".format( archivo_nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55414682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
